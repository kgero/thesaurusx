{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stability of word embeddings\n",
    "\n",
    "Cite: [Evaluating the Stability of Embedding-based Word Similarities](https://transacl.org/ojs/index.php/tacl/article/view/1202/286)\n",
    "\n",
    "Should look into if jaccard sim correlates to pip loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/katy/Documents/Grad/thesaurusx/env/bin/python'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import string\n",
    "\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jaccard similarity\n",
    "\n",
    "Look at overlap in top 10 words across different embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/katy/Documents/Grad/thesaurusx/env/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "darwin 0.3127917833800187\n",
      "observations 0.8787878787878789\n",
      "books 0.5445665445665445\n",
      "open 0.5384615384615384\n",
      "variation 0.7676767676767677\n",
      "cat 0.6666666666666666\n",
      "facts 0.6744366744366744\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_jaccard(embeddings, keystones):\n",
    "    \"\"\"Return dict of keystones: jaccard sim.\n",
    "    \n",
    "    Where jaccard sim is the number of intersecting words/number of unioned words\n",
    "    in words similar to a keystone.\n",
    "    \n",
    "    :param embeddings: list of gensim embedding objects\n",
    "    :param keystones: list of words in vocab to calculate sim on\n",
    "    :return jac_sims: dict where each key is a keystone, each value is the avg jaccard sim\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    for vectors in embeddings:\n",
    "        for key in keystones:\n",
    "            if key in data:\n",
    "                data[key].append(set([w for w,d in vectors.most_similar(key, topn=10)]))\n",
    "            else:\n",
    "                data[key] = [set([w for w, d in vectors.most_similar(key, topn=10)])]\n",
    "\n",
    "    jac_sims = {}\n",
    "    for key in keystones:\n",
    "        jaccard_sims = []\n",
    "        for i, la in enumerate(data[key]):\n",
    "            for lo in data[key][i+1:]:\n",
    "                jac = len(la.intersection(lo)) / len(la.union(lo))\n",
    "                jaccard_sims.append(jac)\n",
    "        jac_sims[key] = sum(jaccard_sims)/len(jaccard_sims)\n",
    "    return jac_sims\n",
    "\n",
    "def load_emb(fnames):\n",
    "    \"\"\"Return list of gensim embedding objects, loaded from filepaths in fnames\"\"\"\n",
    "    vectors = []\n",
    "    for fname in fnames:\n",
    "        vec = gensim.models.KeyedVectors.load_word2vec_format(fname)\n",
    "        print('.', end=' ')\n",
    "        vectors.append(vec)\n",
    "    print('done')\n",
    "    return vectors\n",
    "\n",
    "# darwin = load_emb(['../dat/vecs/{}_{}.txt'.format('darwin', i) for i in range(5)])\n",
    "# oneb = load_emb(['../dat/vecs/{}_{}.txt'.format('oneb', i) for i in range(3)])\n",
    "keystones = ['darwin', 'observations', 'books', 'open', 'variation', 'cat', 'facts']\n",
    "\n",
    "jac_sims = get_jaccard(oneb, keystones)\n",
    "for key in jac_sims:\n",
    "    print(key, jac_sims[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". . . . done\n"
     ]
    }
   ],
   "source": [
    "scifi = load_emb(['../dat/vecs/{}_{}.txt'.format('scifi_100kb', i) for i in range(4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['prank', 'crowded', 'trappings', 'ceasefire', 'demonstrate', 'hymn', 'maniac', 'beginning', 'plenty', 'preconceived']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12440"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_macthes(filepath='../dat/other/mac_thes.txt'):\n",
    "    \"\"\"Return dict of word: set of synonyms.\n",
    "    \n",
    "    Thsi thesaurus is split by pos; ignore for now and throw all\n",
    "    synoyms into same entry.\n",
    "    \"\"\"\n",
    "    thes = dict()\n",
    "    with open(filepath, 'r') as fle:\n",
    "        for line in fle:\n",
    "            entry, words = line.strip('\\n').split(':')\n",
    "            key, pos = entry.split(' ')\n",
    "            words = words.split(', ')\n",
    "            if key in thes:\n",
    "                thes[key].update(words)\n",
    "            else:\n",
    "                thes[key] = set(words)\n",
    "    return thes\n",
    "\n",
    "ogthes = load_macthes()\n",
    "thes = {}\n",
    "for k in ogthes:\n",
    "    if '-' in k:\n",
    "        w = k.replace('-', '')\n",
    "        thes[w] = ogthes[k]\n",
    "    else:\n",
    "        thes[k] = ogthes[k]\n",
    "\n",
    "shared_vocab = set.intersection(set(thes.keys()), get_vocab(scifi[0]))\n",
    "print(list(shared_vocab)[:10])\n",
    "len(shared_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "local_vocab = random.sample(shared_vocab, 5000)\n",
    "l = len(local_vocab)\n",
    "print(l)\n",
    "jac_sims = get_jaccard(scifi, list(local_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3308662354501364, 0.23359305955766385)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jac_sims_array = np.zeros(l)\n",
    "for i, k in enumerate(jac_sims.keys()):\n",
    "    jac_sims_array[i] = jac_sims[k]\n",
    "np.mean(jac_sims_array), np.std(jac_sims_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('gushing,', False)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_vocab = True\n",
    "while in_vocab:\n",
    "    w = random.choice(list(thes.keys()))\n",
    "    in_vocab = w in oneb[0].vocab\n",
    "w, w in oneb[0].vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". . . done\n",
      ". . . done\n"
     ]
    }
   ],
   "source": [
    "joyce = load_emb(['../dat/vecs/{}_{}.txt'.format('joyce', i) for i in range(3)])\n",
    "joyce_lemma = load_emb(['../dat/vecs/{}_{}.txt'.format('joyce_lemma', i) for i in range(3)])      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key            reg emb        lemma emb      \n",
      "love           0.133          0.337          \n",
      "uniform        0.000          0.035          \n",
      "labour         0.000          0.000          \n",
      "open           0.055          0.306          \n",
      "receive        0.035          0.092          \n",
      "cat            0.000          0.055          \n",
      "fact           0.204          0.113          \n"
     ]
    }
   ],
   "source": [
    "keystones = ['love', 'uniform', 'labour', 'open', 'receive', 'cat', 'fact']\n",
    "\n",
    "jac_sims = get_jaccard(joyce, keystones)\n",
    "jac_sims_l = get_jaccard(joyce_lemma, keystones)\n",
    "print('{:<15}{:<15}{:<15}'.format('key', 'reg emb', 'lemma emb'))\n",
    "for key in keystones:\n",
    "    print('{:<15}{:<15.3f}{:<15.3f}'.format(key, jac_sims[key], jac_sims_l[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(emb):\n",
    "    \"\"\"Return vocab of gensim emb object as set.\"\"\"\n",
    "    return set(emb.vocab.keys())\n",
    "\n",
    "def compare_jaccard(emb1, emb2, thes=None):\n",
    "    \"\"\"Return array of jaccard sim for all shared vocab in two sets of emb fles.\n",
    "    \n",
    "    If thes is included, only do words in the thes.\n",
    "    \n",
    "    vocab in all emb1 embeddings should be the same\n",
    "    vocab in all emb2 embeddings should be the same\n",
    "    \n",
    "    :param emb1: list of gensim embedding objs from same data + algo (diff runs)\n",
    "    :param emb2: like above, but different algo (same data! diff runs)\n",
    "    :return: array of jaccard sim for each word in shared vocab\n",
    "    \"\"\"\n",
    "    if thes:\n",
    "        shared = list(set.intersection(*[get_vocab(emb1[0]), get_vocab(emb2[0]), set(thes.keys())]))\n",
    "    else:\n",
    "        shared = list(set.intersection(*[get_vocab(emb1[0]), get_vocab(emb2[0])]))\n",
    "    print('shared vocab', len(shared))\n",
    "    jac1_d = get_jaccard(emb1, shared)\n",
    "    jac2_d = get_jaccard(emb2, shared)\n",
    "    jac1_a = np.zeros(len(shared))\n",
    "    jac2_a = np.zeros(len(shared))\n",
    "    \n",
    "    for i, w in enumerate(shared):\n",
    "        jac1_a[i] = jac1_d[w]\n",
    "        jac2_a[i] = jac2_d[w]\n",
    "    return jac1_a, jac2_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". . . done\n",
      ". . . done\n"
     ]
    }
   ],
   "source": [
    "arxiv = load_emb(['../dat/vecs/{}_{}.txt'.format('arxiv_abs', i) for i in range(3)])\n",
    "arxiv_lemma = load_emb(['../dat/vecs/{}_{}.txt'.format('arxiv_abs_lemma', i) for i in range(3)])      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shared vocab 4854\n",
      "0.317 (0.265), 0.372 (0.262)\n"
     ]
    }
   ],
   "source": [
    "reg, lem = compare_jaccard(arxiv, arxiv_lemma, thes)\n",
    "print('{:.3f} ({:.3f}), {:.3f} ({:.3f})'.format(np.mean(reg), np.std(reg), np.mean(lem), np.std(lem)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shared vocab 14377\n",
      "0.239 (0.241), 0.268 (0.241)\n"
     ]
    }
   ],
   "source": [
    "areg, alem = compare_jaccard(arxiv, arxiv_lemma)\n",
    "print('{:.3f} ({:.3f}), {:.3f} ({:.3f})'.format(np.mean(areg), np.std(areg), np.mean(alem), np.std(alem)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_piploss(embeddings):\n",
    "    \"\"\"Return array of pip losses between all embeddings.\n",
    "    \n",
    "    :param embeddings: list of gensim emb objs; should have same vocab\n",
    "    :return piploss: array of pip loss values\n",
    "    \"\"\"\n",
    "    \n",
    "    pips = []\n",
    "    shared = get_vocab(embeddings[0])\n",
    "    print('vocab:', len(shared))\n",
    "    for i, vec in enumerate(embeddings):\n",
    "        t0 = time()\n",
    "        m = np.zeros((len(shared), len(vec['the'])))\n",
    "        for j, word in enumerate(shared):\n",
    "            v = vec.word_vec(word)\n",
    "            m[j, :] = v / np.sqrt((np.sum(v**2)))\n",
    "        pip = np.matmul(m, m.transpose())\n",
    "        np.save('tmp_{}.npy'.format(i), pip)\n",
    "        del pip\n",
    "        print('A: took (min)', (time()-t0)/60)\n",
    "    \n",
    "    \n",
    "    piploss = []\n",
    "    num = len(embeddings)\n",
    "    for i in range(num):\n",
    "        for j in range(num):\n",
    "            if j >= i:\n",
    "                break\n",
    "            t0 = time()\n",
    "            pip1 = np.load('tmp_{}.npy'.format(i))\n",
    "            pip2 = np.load('tmp_{}.npy'.format(j))\n",
    "            piploss.append(np.linalg.norm(pip1 - pip2))\n",
    "            print('B: took (min)', (time()-t0)/60)\n",
    "    return np.array(piploss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab: 7084\n",
      "A: took (min) 0.01859915256500244\n",
      "A: took (min) 0.018233267466227214\n",
      "A: took (min) 0.01624951362609863\n",
      "B: took (min) 0.014559221267700196\n",
      "B: took (min) 0.022881698608398438\n",
      "B: took (min) 0.02160645325978597\n",
      "took (min) 0.11323598225911459\n",
      "vocab: 6094\n",
      "A: took (min) 0.01271958351135254\n",
      "A: took (min) 0.016518636544545492\n",
      "A: took (min) 0.013743070761362712\n",
      "B: took (min) 0.009520856539408366\n",
      "B: took (min) 0.013298630714416504\n",
      "B: took (min) 0.014247620105743408\n",
      "took (min) 0.08113964796066284\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "j_pip = get_piploss(joyce)\n",
    "print('took (min)', (time()-t0)/60)\n",
    "t0 = time()\n",
    "jl_pip = get_piploss(joyce_lemma)\n",
    "print('took (min)', (time()-t0)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.034 (0.001), 0.043 (0.001)\n"
     ]
    }
   ],
   "source": [
    "j_pip = j_pip / len(get_vocab(joyce[0]))\n",
    "jl_pip = jl_pip / len(get_vocab(joyce_lemma[0]))\n",
    "print('{:.3f} ({:.3f}), {:.3f} ({:.3f})'.format(np.mean(j_pip), np.std(j_pip), np.mean(jl_pip), np.std(jl_pip)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab: 23299\n",
      "A: took (min) 0.4377033829689026\n",
      "A: took (min) 0.34359745581944784\n",
      "A: took (min) 0.34090091784795123\n",
      "B: took (min) 1.6022106488545735\n",
      "B: took (min) 1.6592345317204793\n",
      "B: took (min) 1.7088287830352784\n",
      "took (min) 6.103492780526479\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "j_pip = get_piploss(arxiv)\n",
    "print('took (min)', (time()-t0)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab: 17455\n",
      "A: took (min) 0.09230528275171916\n",
      "A: took (min) 0.11323066552480061\n",
      "A: took (min) 0.1390928308169047\n",
      "B: took (min) 0.6240556001663208\n",
      "B: took (min) 0.7902705033620199\n",
      "B: took (min) 0.6391923467318217\n",
      "took (min) 2.408839929103851\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "al_pip = get_piploss(arxiv_lemma)\n",
    "print('took (min)', (time()-t0)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_pip = get_piploss(arxiv)\n",
    "al_pip = get_piploss(arxiv_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.088 (0.001), 0.079 (0.002)\n"
     ]
    }
   ],
   "source": [
    "a_pip_n = a_pip / len(get_vocab(arxiv[0]))\n",
    "al_pip_n = al_pip / len(get_vocab(arxiv_lemma[0]))\n",
    "print('{:.3f} ({:.3f}), {:.3f} ({:.3f})'.format(np.mean(a_pip_n), np.std(a_pip_n), np.mean(al_pip_n), np.std(al_pip_n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
