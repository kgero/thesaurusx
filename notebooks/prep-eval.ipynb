{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import spacy\n",
    "\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple apple PROPN NNP  Xxxxx True False\n",
      "is be VERB VBZ  xx True True\n",
      "looking look VERB VBG  xxxx True False\n",
      "at at ADP IN  xx True True\n",
      "buying buy VERB VBG  xxxx True False\n",
      "U.K. u.k. PROPN NNP  X.X. False False\n",
      "startup startup NOUN NN  xxxx True False\n",
      "for for ADP IN  xxx True True\n",
      "$ $ SYM $  $ False False\n",
      "1 1 NUM CD  d False False\n",
      "billion billion NUM CD  xxxx True False\n"
     ]
    }
   ],
   "source": [
    "sentence = \"\"\"At eight o'clock on Thursday morning Arthur didn't feel very good.\"\"\"\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "doc = nlp(u'Apple is looking at buying U.K. startup for $1 billion')\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "          token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We -PRON- PRON PRP\n",
      "propose propose VERB VBP\n",
      "an an DET DT\n",
      "architecture architecture NOUN NN\n",
      "for for ADP IN\n",
      "VQA vqa PROPN NNP\n",
      "which which ADJ WDT\n",
      "utilizes utilize VERB VBZ\n",
      "recurrent recurrent ADJ JJ\n",
      "layers layer NOUN NNS\n",
      "to to PART TO\n",
      "generate generate VERB VB\n",
      "visual visual ADJ JJ\n",
      "and and CCONJ CC\n",
      "textual textual ADJ JJ\n",
      "attention attention NOUN NN\n",
      ". . PUNCT .\n",
      "        SPACE _SP\n",
      "The the DET DT\n",
      "memory memory NOUN NN\n",
      "characteristic characteristic ADJ JJ\n",
      "of of ADP IN\n",
      "the the DET DT\n",
      "proposed propose VERB VBN\n",
      "recurrent recurrent ADJ JJ\n",
      "attention attention NOUN NN\n",
      "units unit NOUN NNS\n",
      "offers offer VERB VBZ\n",
      "a a DET DT\n",
      "rich rich ADJ JJ\n",
      "joint joint ADJ JJ\n",
      "embedding embedding NOUN NN\n",
      "of of ADP IN\n",
      "visual visual ADJ JJ\n",
      "and and CCONJ CC\n",
      "          SPACE _SP\n",
      "textual textual ADJ JJ\n",
      "features feature NOUN NNS\n",
      "and and CCONJ CC\n",
      "enables enable VERB VBZ\n",
      "the the DET DT\n",
      "model model NOUN NN\n",
      "to to ADP IN\n",
      "reason reason NOUN NN\n",
      "relations relation NOUN NNS\n",
      "between between ADP IN\n",
      "several several ADJ JJ\n",
      "parts part NOUN NNS\n",
      "of of ADP IN\n",
      "the the DET DT\n",
      "image image NOUN NN\n",
      "and and CCONJ CC\n",
      "question question NOUN NN\n",
      ". . PUNCT .\n"
     ]
    }
   ],
   "source": [
    "test1 = \"We propose an architecture for VQA which utilizes recurrent layers to generate visual and textual attention.\\\n",
    "    The memory characteristic of the proposed recurrent attention units offers a rich joint embedding of visual and \\\n",
    "    textual features and enables the model to reason relations between several parts of the image and question.\"\n",
    "\n",
    "test2 = \"Our single model outperforms the first place winner on the VQA 1.0 dataset, performs within margin to the \\\n",
    "    current state-of-the-art ensemble model. We also experiment with replacing attention mechanisms in other \\\n",
    "    state-of-the-art models with our implementation and show increased accuracy. In both cases, our recurrent \\\n",
    "    attention mechanism improves performance in tasks requiring sequential or relational reasoning on the VQA \\\n",
    "    dataset.\"\n",
    "\n",
    "test2a = \"Our single model outperforms the first place winner on the VQA 1.0 dataset, performs within margin to the \\\n",
    "    current state-of-the-art ensemble model. We also experiment with replacing attention mechanisms in other \\\n",
    "    state-of-the-art models with our implementation and show increased accuracy. In both cases, our recurrent \\n\\\n",
    "    attention mechanism improves performance in tasks requiring sequential or relational reasoning on the VQA \\n\\\n",
    "    dataset.\"\n",
    "\n",
    "doc = nlp(test1)\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "took 464.2969620227814 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "with open('../dat/corpora/arxiv_abs/summaries.txt', 'r') as fle:\n",
    "    data = {}\n",
    "    for line in fle:  # each line is one abstract\n",
    "        if line == '\\n':\n",
    "            continue\n",
    "        doc = nlp(line.strip('\\n'))\n",
    "        for token in doc:\n",
    "            if token.pos_ not in data:\n",
    "                data[token.pos_] = {}\n",
    "            if token.text in data[token.pos_]:\n",
    "                data[token.pos_][token.text] += 1\n",
    "            else:\n",
    "                data[token.pos_][token.text] = 1\n",
    "print('took', time() - start, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Fourth', 4), ('third', 3), ('second', 2), ('first', 1)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbers = {'first': 1, 'second': 2, 'third': 3, 'Fourth': 4}\n",
    "[(name, numbers[name]) for name in sorted(numbers, key=numbers.__getitem__, reverse=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('data', 29109),\n",
       " ('model', 24215),\n",
       " ('paper', 20998),\n",
       " ('method', 19881),\n",
       " ('algorithm', 17827),\n",
       " ('learning', 17253),\n",
       " ('problem', 16990),\n",
       " ('approach', 16269),\n",
       " ('results', 16083),\n",
       " ('methods', 15960),\n",
       " ('image', 15666),\n",
       " ('models', 15176),\n",
       " ('network', 14210),\n",
       " ('performance', 13778),\n",
       " ('information', 11804),\n",
       " ('time', 11643),\n",
       " ('state', 11305),\n",
       " ('algorithms', 11244),\n",
       " ('images', 10876),\n",
       " ('networks', 10602),\n",
       " ('features', 9826),\n",
       " ('classification', 9467),\n",
       " ('system', 9415),\n",
       " ('training', 9115),\n",
       " ('art', 8992),\n",
       " ('framework', 8918),\n",
       " ('problems', 8559),\n",
       " ('number', 8318),\n",
       " ('analysis', 8275),\n",
       " ('task', 8076),\n",
       " ('work', 7784),\n",
       " ('accuracy', 6964),\n",
       " ('feature', 6811),\n",
       " ('tasks', 6754),\n",
       " ('function', 6732),\n",
       " ('recognition', 6517),\n",
       " ('systems', 6467),\n",
       " ('detection', 6425),\n",
       " ('set', 6334),\n",
       " ('dataset', 6328),\n",
       " ('datasets', 6321),\n",
       " ('space', 6016),\n",
       " ('language', 5934),\n",
       " ('order', 5906),\n",
       " ('optimization', 5856),\n",
       " ('approaches', 5803),\n",
       " ('machine', 5790),\n",
       " ('%', 5766),\n",
       " ('knowledge', 5759),\n",
       " ('applications', 5676)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loc_data = data['NOUN']\n",
    "[(word, loc_data[word]) for word in sorted(loc_data, key=loc_data.__getitem__, reverse=True)][:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(data, open('science-counts.pkl', 'wb'))\n",
    "# pos_d = pickle.load(open('../dat/pos/arxiv_abs.pkl', 'rb'))\n",
    "# pos_d['propose']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata = pickle.load(open('science-counts.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82001\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'We propose an architecture for VQA which utilizes recurrent layers to generate visual and textual attention. The memory characteristic of the proposed recurrent attention units offers a rich joint embedding of visual and textual features and enables the model to reason relations between several parts of the image and question. Our single model outperforms the first place winner on the VQA 1.0 dataset, performs within margin to the current state-of-the-art ensemble model. We also experiment with replacing attention mechanisms in other state-of-the-art models with our implementation and show increased accuracy. In both cases, our recurrent attention mechanism improves performance in tasks requiring sequential or relational reasoning on the VQA dataset.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../dat/corpora/arxiv_abs/summaries.txt', 'r') as fle:\n",
    "    abstracts = fle.read()\n",
    "\n",
    "abstracts_ = abstracts.split('\\n')\n",
    "print(len(abstracts_))\n",
    "abstracts_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(test1)\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
