{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation style thesauruses and underlying embeddings\n",
    "\n",
    "1. Look at word-level PIP loss\n",
    "2. Look at overlap between simple_lookup and synonym/related word lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings from dat/annoy directory...\n",
      "EMBEDDINGS:\n",
      " dict_keys(['word2vec-slim', 'poetry', 'australia', 'joyce', 'oneb', 'dickens', 'law', 'joyce-dep', 'merge-science-big', 'darwin100dep', 'darwin_0', 'darwin', 'darwin_1', 'gandhi', 'darwin_3', 'nyt-science', 'darwin_2', 'darwin_4', 'aretha', 'sherlock', 'arxiv_abs', 'merge-science-small', 'glove-slim'])\n",
      "VOCAB:\n",
      " dict_keys(['glove-slim', 'foodreviews', 'oneb', 'poetry', 'gandhi', 'nyt-science', 'darwin', 'joyce-dep', 'darwin_1', 'aretha', 'darwin_3', 'darwin100dep', 'dickens', 'merge-science-small', 'arxiv_abs', 'darwin_2', 'joyce', 'word2vec-slim', 'australia', 'darwin_0', 'law', 'darwin_4', 'sherlock', 'merge-science-big'])\n",
      "PART OF SPEECH:\n",
      " dict_keys(['merge-science-big', 'dickens', 'joyce', 'poetry', 'merge-science-small', 'arxiv_abs', 'sherlock', 'nyt-science', 'darwin', 'gandhi'])\n",
      "Ready!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import requests\n",
    "import spacy\n",
    "\n",
    "from time import time\n",
    "\n",
    "from src.simple_lookup import simple_lookup, get_pos, get_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['present',\n",
       " 'develop',\n",
       " 'devise',\n",
       " 'describe',\n",
       " 'adopt',\n",
       " 'employ',\n",
       " 'formulate',\n",
       " 'advocate',\n",
       " 'explore',\n",
       " 'extend']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_lookup('propose', 'VERB', embkey=\"arxiv_abs\")['words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". . . . "
     ]
    }
   ],
   "source": [
    "base_path = 'dat/vecs/'\n",
    "embd_nmes = ['glove-slim', 'word2vec-slim', 'arxiv_abs', 'nyt-science']\n",
    "embd_vecs = []\n",
    "for nme in embd_nmes:\n",
    "    fle = base_path + nme + '.txt'\n",
    "    vec = gensim.models.KeyedVectors.load_word2vec_format(fle)\n",
    "    embd_vecs.append(vec)\n",
    "    print('.', end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at PIP loss\n",
    "\n",
    "Function for single word PIP loss, both local and global. Look at difference between lots of embeddings for a single word, or between just two embeddings for a bunch of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(v):\n",
    "    return v / np.sqrt((np.sum(v**2)))\n",
    "\n",
    "def pip_singleword(response, embeddings, emb_names, local=False):\n",
    "    \"\"\"Return dict of PIP 'vector' for response word; one for each embedding.\n",
    "    \n",
    "    If local, PIP matrix is only over top 50 words nearest to response.\n",
    "        (Has to be shared in all embeddings)\n",
    "    Else PIP matrix is over shared vocab words across all embeddings.\n",
    "    \n",
    "    PIP 'vector' is the distance from response to every other word in relevant vocab.\n",
    "    \n",
    "    :param response: word to calculate distances from\n",
    "    :param embeddings: list of gensim embeddings\n",
    "    :param emb_names: list of names of gensim embeddings\n",
    "    :param local: boolean indicator\n",
    "    :return pips: dict, key is embedding name, value is pip vector\n",
    "    \"\"\"\n",
    "    vocabs = []\n",
    "    for emb, nme in zip(embeddings, emb_names):\n",
    "        if response not in emb.vocab:\n",
    "            raise ValueError('response word not in {} embedding vocab'.format(nme))\n",
    "        if local:\n",
    "            vocabs.append(set([w for w, d in emb.most_similar(response, topn=50)]))\n",
    "        else:\n",
    "            vocabs.append(set(emb.vocab))\n",
    "    \n",
    "    shared_vocab = list(set.intersection(*vocabs))\n",
    "    \n",
    "    pips = {}\n",
    "    for emb, nme in zip(embeddings, emb_names):\n",
    "        pip = np.zeros(len(shared_vocab))\n",
    "        for i, w in enumerate(shared_vocab):\n",
    "            pip[i] = np.dot(norm(emb[response]), norm(emb[w]))\n",
    "        pips[nme] = pip\n",
    "    \n",
    "    return pips\n",
    "\n",
    "def get_piploss(pips, emb1, emb2):\n",
    "    \"\"\"Return pip loss between pips[emb1] and pips[emb2].\n",
    "    \n",
    "    :param pips: dict of emb_name: PIP vector for a single word (precalculated)\n",
    "    :param emb1: key for pips dict\n",
    "    :param emb2: key for pips dict\n",
    "    :return: float\n",
    "    \"\"\"\n",
    "    return np.linalg.norm(pips[emb1] - pips[emb2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('glove-slim | glove-slim', 0.0),\n",
       " ('word2vec-slim | word2vec-slim', 0.0),\n",
       " ('arxiv_abs | arxiv_abs', 0.0),\n",
       " ('nyt-science | nyt-science', 0.0),\n",
       " ('word2vec-slim | arxiv_abs', 18.993049814164067),\n",
       " ('word2vec-slim | nyt-science', 22.033235389497364),\n",
       " ('arxiv_abs | nyt-science', 23.46319974984484),\n",
       " ('glove-slim | word2vec-slim', 25.357969622687676),\n",
       " ('glove-slim | arxiv_abs', 33.35568080684847),\n",
       " ('glove-slim | nyt-science', 33.46942931278002)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = 'model'\n",
    "output = []\n",
    "for i, (vec, nme) in enumerate(zip(embd_vecs, embd_nmes)):\n",
    "    for vec1, nme1 in zip(embd_vecs[i:], embd_nmes[i:]):\n",
    "        pips = pip_singleword(word, [vec, vec1], [nme, nme1])\n",
    "        output.append((nme+' | '+nme1, get_piploss(pips, nme, nme1)))\n",
    "\n",
    "sorted(output, key=lambda tup: tup[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/katy/Documents/Grad/thesaurusx/env/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('glove-slim | glove-slim', 0.0),\n",
       " ('word2vec-slim | word2vec-slim', 0.0),\n",
       " ('arxiv_abs | arxiv_abs', 0.0),\n",
       " ('nyt-science | nyt-science', 0.0),\n",
       " ('glove-slim | arxiv_abs', 0.21182660611786047),\n",
       " ('arxiv_abs | nyt-science', 0.31366543889324544),\n",
       " ('glove-slim | nyt-science', 0.31642927957772454),\n",
       " ('word2vec-slim | arxiv_abs', 0.5275869984961908),\n",
       " ('word2vec-slim | nyt-science', 0.799887196476512),\n",
       " ('glove-slim | word2vec-slim', 0.8978041714705951)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = 'model'\n",
    "output = []\n",
    "for i, (vec, nme) in enumerate(zip(embd_vecs, embd_nmes)):\n",
    "    for vec1, nme1 in zip(embd_vecs[i:], embd_nmes[i:]):\n",
    "        pips = pip_singleword(word, [vec, vec1], [nme, nme1], local=True)\n",
    "        output.append((nme+' | '+nme1, get_piploss(pips, nme, nme1)))\n",
    "\n",
    "sorted(output, key=lambda tup: tup[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11152"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get shared vocab\n",
    "vocabs = []\n",
    "for emb, nme in zip(embd_vecs, embd_nmes):\n",
    "    vocabs.append(set(emb.vocab))\n",
    "shared_vocab = set.intersection(*vocabs)\n",
    "len(shared_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['glove-slim', 'word2vec-slim', 'arxiv_abs', 'nyt-science']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embd_nmes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['arxiv_abs', 'glove-slim']\n",
      "answered       18.2\n",
      "textures       18.5\n",
      "enrich         18.8\n",
      "magnitudes     19.8\n",
      "mutually       19.9\n",
      "satisfied      20.1\n",
      "temperature    20.3\n",
      "shattering     21.4\n",
      "photographs    22.5\n",
      "mimics         24.0\n",
      "projection     24.4\n",
      "scene          25.0\n",
      "principles     25.7\n",
      "conventions    28.0\n",
      "domination     28.6\n",
      "disclosed      30.1\n",
      "translators    30.8\n",
      "swarms         31.0\n",
      "abruptly       31.5\n",
      "allow          32.3\n",
      "dermatologists 32.5\n",
      "followed       32.7\n",
      "because        34.5\n",
      "diminished     34.6\n",
      "uncontrollable 35.1\n",
      "funny          35.1\n",
      "leg            35.4\n",
      "professionally 36.7\n",
      "chi            42.2\n",
      "dec            44.4\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision=3)\n",
    "\n",
    "n = 30\n",
    "words = random.sample(shared_vocab, n)\n",
    "names = [embd_nmes[2], embd_nmes[0]]\n",
    "vecs = [embd_vecs[2], embd_vecs[0]]\n",
    "print(names)\n",
    "\n",
    "piploss = []\n",
    "for word in words:\n",
    "    pips = pip_singleword(word, embd_vecs, embd_nmes)\n",
    "    piploss.append(get_piploss(pips, names[0], names[1]))\n",
    "piploss = np.array(piploss)\n",
    "\n",
    "id_ord = np.argsort(piploss)\n",
    "for i in id_ord:\n",
    "    print('{:<15}{:.1f}'.format(words[i], piploss[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at similarity to synonym and related words lists\n",
    "\n",
    "Using macos thesaurus as synonym list and moby thesaurus as related word list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_macthes(filepath='dat/other/mac_thes.txt'):\n",
    "    \"\"\"Return dict of word: set of synonyms.\n",
    "    \n",
    "    Thsi thesaurus is split by pos; ignore for now and throw all\n",
    "    synoyms into same entry.\n",
    "    \"\"\"\n",
    "    thes = dict()\n",
    "    with open(filepath, 'r') as fle:\n",
    "        for line in fle:\n",
    "            entry, words = line.strip('\\n').split(':')\n",
    "            key, pos = entry.split(' ')\n",
    "            words = words.split(', ')\n",
    "            if key in thes:\n",
    "                thes[key].update(words)\n",
    "            else:\n",
    "                thes[key] = set(words)\n",
    "    return thes\n",
    "\n",
    "def load_mobythes(filepath='dat/other/mthesaur.txt'):\n",
    "    \"\"\"Return dict of word: set of synonyms.\"\"\"\n",
    "    thes = dict()\n",
    "    with open(filepath, 'r') as fle:\n",
    "        for line in fle:\n",
    "            words = line.strip('\\n').split(',')\n",
    "            thes[words[0]] = set(words[1:])\n",
    "    return thes\n",
    "\n",
    "def get_thes_count(key, words, thes):\n",
    "    \"\"\"Return num of words in words that are 'synoyms' of key according to thes.\n",
    "    \n",
    "    :param key: str\n",
    "    :param words: list of str\n",
    "    :param thes: dict of str to list of str\n",
    "    :return count: int\n",
    "    \"\"\"\n",
    "    if key not in thes:\n",
    "        raise ValueError('key ({}) not in thes'.format(key))\n",
    "    syns = thes[key]\n",
    "    count = 0\n",
    "    for w in words:\n",
    "        if w in syns:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "mac = load_macthes()\n",
    "mob = load_mobythes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mac thes DETER 27 entries:\n",
      "hinder, demoralize, check, inhibit, stave off, fend off, avert, disincentivize, obstruct, stop, impede, scare off, prevent, dissuade, halt, block, foil, forestall, intimidate, daunt, hamper, put off, discourage, counteract, ward off, curb, dishearten, \n",
      "\n",
      "mob thes DETER 64 entries:\n",
      "estop, hinder, check, disinterest, faze, bar, debar, scare, wean from, preclude, inhibit, stave off, obviate, awe, fend off, foreclose, deflect, avert, restrain, damp, turn from, quench, obstruct, frighten, keep from, repel, turn aside, disincline, overawe, shake, dampen, exclude, impede, scare off, stop, prevent, ward, turn off, dissuade, shut out, save, fend, block, blunt, prohibit, indispose, forestall, daunt, intimidate, put off, cool, distract, divert, anticipate, turn away, help, chill, discourage, forbid, keep off, ward off, disaffect, dishearten, rule out, "
     ]
    }
   ],
   "source": [
    "key = 'deter'\n",
    "print('mac thes', key.upper(), len(mac[key]), 'entries:')\n",
    "for w in mac[key]:\n",
    "    print(w, end=', ')\n",
    "print('\\n\\nmob thes', key.upper(), len(mob[key]), 'entries:')\n",
    "for w in mob[key]:\n",
    "    print(w, end=', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['glove-slim', 'word2vec-slim', 'arxiv_abs', 'nyt-science'],\n",
       " [<gensim.models.keyedvectors.Word2VecKeyedVectors at 0x10eb5c860>,\n",
       "  <gensim.models.keyedvectors.Word2VecKeyedVectors at 0x16dcdcc88>,\n",
       "  <gensim.models.keyedvectors.Word2VecKeyedVectors at 0x1771f12e8>,\n",
       "  <gensim.models.keyedvectors.Word2VecKeyedVectors at 0x17791be80>])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embd_nmes, embd_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['models', 'design', 'concept', 'prototype', 'introduced', 'example', 'type', 'developed', 'same', 'version']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key = 'model'\n",
    "response = [w for w, d in embd_vecs[0].most_similar(key, topn=10)]\n",
    "print(response)\n",
    "get_thes_count(key, response, mac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 4., 0., 1., 6., 4., 0., 1., 1., 0., 1., 3., 0., 2., 4., 0., 5.,\n",
       "       0., 2., 1., 1., 3., 3., 0., 0., 1., 0., 1., 2., 0.])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_vocab(emb):\n",
    "    \"\"\"Return vocab of gensim emb object as set.\"\"\"\n",
    "    return set(emb.vocab.keys())\n",
    "\n",
    "def get_syn_meas(emb, thes, n=30):\n",
    "    \"\"\"Randomly select n words from emb and return array of get_thes_count from top 10.\n",
    "    \n",
    "    :param emb: gensim emb ob\n",
    "    :param thes: dict of words to list of syn\n",
    "    :param n: number of words from emb vocab to randomly sample\n",
    "    :return counts: np array\n",
    "    \"\"\"\n",
    "    vocab = list(get_vocab(emb))\n",
    "    words = random.sample(vocab, n)\n",
    "    counts = np.zeros(n)\n",
    "    for i, w in enumerate(words):\n",
    "        while w not in thes:\n",
    "            w = random.sample(vocab, 1)[0]\n",
    "        response = [w for w, d in emb.most_similar(w, topn=10)]\n",
    "        counts[i] = get_thes_count(w, response, thes)\n",
    "    return counts\n",
    "\n",
    "get_syn_meas(embd_vecs[0], mob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove-slim     1.349          1.474          \n",
      "word2vec-slim  1.749          2.074          \n",
      "arxiv_abs      0.362          0.628          \n",
      "nyt-science    0.572          0.715          \n",
      "arxiv_abs_lem  0.466          0.650          \n"
     ]
    }
   ],
   "source": [
    "for emb, nme in zip(embd_vecs, embd_nmes):\n",
    "    syns = get_syn_meas(emb, mac, n=1000)\n",
    "    rels = get_syn_meas(emb, mob, n=1000)\n",
    "    print('{:<15}{:<15.3f}{:<15.3f}'.format(nme, np.mean(syns), np.mean(rels)))\n",
    "\n",
    "emb = gensim.models.KeyedVectors.load_word2vec_format('dat/vecs/arxiv_abs_lemma.txt')\n",
    "syns = get_syn_meas(emb, mac, n=1000)\n",
    "rels = get_syn_meas(emb, mob, n=1000)\n",
    "print('{:<15}{:<15.3f}{:<15.3f}'.format('arxiv_abs_lem', np.mean(syns), np.mean(rels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'poetry', 'australia', 'joyce', 'oneb', 'dickens', 'law', 'joyce-dep', 'merge-science-big', 'darwin100dep', 'darwin_0', 'darwin', 'darwin_1', 'gandhi', 'darwin_3', 'nyt-science', 'darwin_2', 'darwin_4', 'aretha', 'sherlock', 'arxiv_abs', 'merge-science-small', 'glove-slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
