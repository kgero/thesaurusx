{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation style thesauruses and underlying embeddings\n",
    "\n",
    "1. Look at word-level PIP loss\n",
    "2. Look at overlap between simple_lookup and synonym/related word lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import requests\n",
    "import spacy\n",
    "\n",
    "from time import time\n",
    "\n",
    "from src.simple_lookup import simple_lookup, get_pos, get_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['present',\n",
       " 'develop',\n",
       " 'devise',\n",
       " 'describe',\n",
       " 'adopt',\n",
       " 'employ',\n",
       " 'formulate',\n",
       " 'advocate',\n",
       " 'explore',\n",
       " 'extend']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_lookup('propose', 'VERB', embkey=\"arxiv_abs\")['words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". . . . . "
     ]
    }
   ],
   "source": [
    "base_path = 'dat/vecs/'\n",
    "embd_nmes = ['glove-slim', 'word2vec-slim', 'arxiv_abs', 'nyt-science', 'arxiv_abs_lemma']\n",
    "embd_vecs = []\n",
    "for nme in embd_nmes:\n",
    "    fle = base_path + nme + '.txt'\n",
    "    vec = gensim.models.KeyedVectors.load_word2vec_format(fle)\n",
    "    embd_vecs.append(vec)\n",
    "    print('.', end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get size of all corpuses\n",
    "\n",
    "corpus | words\n",
    "----- | ------\n",
    "oneb | 768,648,884 words\n",
    "arxiv-category-corpora | 546,695,438 words\n",
    " food | 47,184,823 words\n",
    " nyt-science | 31,375,098 words\n",
    "scifi | 26,656,594 words\n",
    " gandhi | 13,450,085 words\n",
    "  arxiv_abs | 6,220,371 words\n",
    "  dickens | 6,033,912 words\n",
    "   darwin | 3,088,979 words\n",
    "   law | 2,721,025 words\n",
    "    poetry | 1,302,139 words\n",
    "sherlock | 857,351 words\n",
    " joyce | 432,819 words\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dat/corpora/sherlock\n",
      "sherlock : 857351 words\n",
      "dat/corpora/gandhi\n",
      "gandhi : 13450085 words\n",
      "dat/corpora/dickens\n",
      "dickens : 6033912 words\n",
      "dat/corpora/merge-science-big\n",
      "merge-science-big : 34583381 words\n",
      "dat/corpora/law\n",
      "law : 2721025 words\n",
      "dat/corpora/nyt\n",
      "nyt : 36832892 words\n",
      "dat/corpora/merge-science-small\n",
      "merge-science-small : 14554458 words\n",
      "dat/corpora/poetry\n",
      "poetry : 1302139 words\n",
      "dat/corpora/nyt-science\n",
      "nyt-science : 31375098 words\n",
      "dat/corpora/food\n",
      "food : 47184823 words\n",
      "dat/corpora/darwin\n",
      "darwin : 3088979 words\n",
      "dat/corpora/oneb\n",
      "oneb : 768648884 words\n",
      "dat/corpora/joyce\n",
      "joyce : 432819 words\n",
      "dat/corpora/arxiv-category-corpora-master\n",
      "arxiv-category-corpora-master : 546695438 words\n",
      "dat/corpora/arxiv_abs\n",
      "arxiv_abs : 6220371 words\n"
     ]
    }
   ],
   "source": [
    "base = 'dat/corpora/'\n",
    "for directory in os.listdir(base):\n",
    "    thispath = os.path.join(base, directory)\n",
    "    if os.path.isdir(thispath):\n",
    "        print(thispath)\n",
    "        word_cnt = 0\n",
    "        for fname in os.listdir(thispath):\n",
    "            for lne in open(os.path.join(thispath, fname), errors='ignore'):\n",
    "                word_cnt += len(lne.split(' '))\n",
    "        print(directory, \":\", word_cnt, 'words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at PIP loss\n",
    "\n",
    "Function for single word PIP loss, both local and global. Look at difference between lots of embeddings for a single word, or between just two embeddings for a bunch of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(v):\n",
    "    return v / np.sqrt((np.sum(v**2)))\n",
    "\n",
    "def pip_singleword(response, embeddings, emb_names, local=False):\n",
    "    \"\"\"Return dict of PIP 'vector' for response word; one for each embedding.\n",
    "    \n",
    "    If local, PIP matrix is only over top 50 words nearest to response.\n",
    "        (Has to be shared in all embeddings)\n",
    "    Else PIP matrix is over shared vocab words across all embeddings.\n",
    "    \n",
    "    PIP 'vector' is the distance from response to every other word in relevant vocab.\n",
    "    \n",
    "    :param response: word to calculate distances from\n",
    "    :param embeddings: list of gensim embeddings\n",
    "    :param emb_names: list of names of gensim embeddings\n",
    "    :param local: boolean indicator\n",
    "    :return pips: dict, key is embedding name, value is pip vector\n",
    "    \"\"\"\n",
    "    vocabs = []\n",
    "    for emb, nme in zip(embeddings, emb_names):\n",
    "        if response not in emb.vocab:\n",
    "            raise ValueError('response word not in {} embedding vocab'.format(nme))\n",
    "        if local:\n",
    "            vocabs.append(set([w for w, d in emb.most_similar(response, topn=50)]))\n",
    "        else:\n",
    "            vocabs.append(set(emb.vocab))\n",
    "    \n",
    "    shared_vocab = list(set.intersection(*vocabs))\n",
    "    \n",
    "    pips = {}\n",
    "    for emb, nme in zip(embeddings, emb_names):\n",
    "        pip = np.zeros(len(shared_vocab))\n",
    "        for i, w in enumerate(shared_vocab):\n",
    "            pip[i] = np.dot(norm(emb[response]), norm(emb[w]))\n",
    "        pips[nme] = pip\n",
    "    \n",
    "    return pips\n",
    "\n",
    "def get_piploss(pips, emb1, emb2):\n",
    "    \"\"\"Return pip loss between pips[emb1] and pips[emb2].\n",
    "    \n",
    "    :param pips: dict of emb_name: PIP vector for a single word (precalculated)\n",
    "    :param emb1: key for pips dict\n",
    "    :param emb2: key for pips dict\n",
    "    :return: float\n",
    "    \"\"\"\n",
    "    return np.linalg.norm(pips[emb1] - pips[emb2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('glove-slim | glove-slim', 0.0),\n",
       " ('word2vec-slim | word2vec-slim', 0.0),\n",
       " ('arxiv_abs | arxiv_abs', 0.0),\n",
       " ('nyt-science | nyt-science', 0.0),\n",
       " ('word2vec-slim | arxiv_abs', 18.993049814164067),\n",
       " ('word2vec-slim | nyt-science', 22.033235389497364),\n",
       " ('arxiv_abs | nyt-science', 23.46319974984484),\n",
       " ('glove-slim | word2vec-slim', 25.357969622687676),\n",
       " ('glove-slim | arxiv_abs', 33.35568080684847),\n",
       " ('glove-slim | nyt-science', 33.46942931278002)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = 'model'\n",
    "output = []\n",
    "for i, (vec, nme) in enumerate(zip(embd_vecs, embd_nmes)):\n",
    "    for vec1, nme1 in zip(embd_vecs[i:], embd_nmes[i:]):\n",
    "        pips = pip_singleword(word, [vec, vec1], [nme, nme1])\n",
    "        output.append((nme+' | '+nme1, get_piploss(pips, nme, nme1)))\n",
    "\n",
    "sorted(output, key=lambda tup: tup[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/katy/Documents/Grad/thesaurusx/env/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('glove-slim | glove-slim', 0.0),\n",
       " ('word2vec-slim | word2vec-slim', 0.0),\n",
       " ('arxiv_abs | arxiv_abs', 0.0),\n",
       " ('nyt-science | nyt-science', 0.0),\n",
       " ('glove-slim | arxiv_abs', 0.21182660611786047),\n",
       " ('arxiv_abs | nyt-science', 0.31366543889324544),\n",
       " ('glove-slim | nyt-science', 0.31642927957772454),\n",
       " ('word2vec-slim | arxiv_abs', 0.5275869984961908),\n",
       " ('word2vec-slim | nyt-science', 0.799887196476512),\n",
       " ('glove-slim | word2vec-slim', 0.8978041714705951)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = 'model'\n",
    "output = []\n",
    "for i, (vec, nme) in enumerate(zip(embd_vecs, embd_nmes)):\n",
    "    for vec1, nme1 in zip(embd_vecs[i:], embd_nmes[i:]):\n",
    "        pips = pip_singleword(word, [vec, vec1], [nme, nme1], local=True)\n",
    "        output.append((nme+' | '+nme1, get_piploss(pips, nme, nme1)))\n",
    "\n",
    "sorted(output, key=lambda tup: tup[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11152"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get shared vocab\n",
    "vocabs = []\n",
    "for emb, nme in zip(embd_vecs, embd_nmes):\n",
    "    vocabs.append(set(emb.vocab))\n",
    "shared_vocab = set.intersection(*vocabs)\n",
    "len(shared_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['glove-slim', 'word2vec-slim', 'arxiv_abs', 'nyt-science']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embd_nmes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at PIP loss between two embeddings for a random sample of words of *n* words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['arxiv_abs', 'glove-slim']\n",
      "answered       18.2\n",
      "textures       18.5\n",
      "enrich         18.8\n",
      "magnitudes     19.8\n",
      "mutually       19.9\n",
      "satisfied      20.1\n",
      "temperature    20.3\n",
      "shattering     21.4\n",
      "photographs    22.5\n",
      "mimics         24.0\n",
      "projection     24.4\n",
      "scene          25.0\n",
      "principles     25.7\n",
      "conventions    28.0\n",
      "domination     28.6\n",
      "disclosed      30.1\n",
      "translators    30.8\n",
      "swarms         31.0\n",
      "abruptly       31.5\n",
      "allow          32.3\n",
      "dermatologists 32.5\n",
      "followed       32.7\n",
      "because        34.5\n",
      "diminished     34.6\n",
      "uncontrollable 35.1\n",
      "funny          35.1\n",
      "leg            35.4\n",
      "professionally 36.7\n",
      "chi            42.2\n",
      "dec            44.4\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision=3)\n",
    "\n",
    "n = 30\n",
    "words = random.sample(shared_vocab, n)\n",
    "names = [embd_nmes[2], embd_nmes[0]]\n",
    "vecs = [embd_vecs[2], embd_vecs[0]]\n",
    "print(names)\n",
    "\n",
    "piploss = []\n",
    "for word in words:\n",
    "    pips = pip_singleword(word, embd_vecs, embd_nmes)\n",
    "    piploss.append(get_piploss(pips, names[0], names[1]))\n",
    "piploss = np.array(piploss)\n",
    "\n",
    "id_ord = np.argsort(piploss)\n",
    "for i in id_ord:\n",
    "    print('{:<15}{:.1f}'.format(words[i], piploss[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at similarity to synonym and related words lists\n",
    "\n",
    "Using macos thesaurus as synonym list and moby thesaurus as related word list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_macthes(filepath='dat/other/mac_thes.txt'):\n",
    "    \"\"\"Return dict of word: set of synonyms.\n",
    "    \n",
    "    Thsi thesaurus is split by pos; ignore for now and throw all\n",
    "    synoyms into same entry.\n",
    "    \"\"\"\n",
    "    thes = dict()\n",
    "    with open(filepath, 'r') as fle:\n",
    "        for line in fle:\n",
    "            entry, words = line.strip('\\n').split(':')\n",
    "            key, pos = entry.split(' ')\n",
    "            words = words.split(', ')\n",
    "            if key in thes:\n",
    "                thes[key].update(words)\n",
    "            else:\n",
    "                thes[key] = set(words)\n",
    "    return thes\n",
    "\n",
    "def load_mobythes(filepath='dat/other/mthesaur.txt'):\n",
    "    \"\"\"Return dict of word: set of synonyms.\"\"\"\n",
    "    thes = dict()\n",
    "    with open(filepath, 'r') as fle:\n",
    "        for line in fle:\n",
    "            words = line.strip('\\n').split(',')\n",
    "            thes[words[0]] = set(words[1:])\n",
    "    return thes\n",
    "\n",
    "def get_thes_count(key, words, thes):\n",
    "    \"\"\"Return num of words in words that are 'synoyms' of key according to thes.\n",
    "    \n",
    "    :param key: str\n",
    "    :param words: list of str\n",
    "    :param thes: dict of str to list of str\n",
    "    :return count: int\n",
    "    \"\"\"\n",
    "    if key not in thes:\n",
    "        raise ValueError('key ({}) not in thes'.format(key))\n",
    "    syns = thes[key]\n",
    "    count = 0\n",
    "    for w in words:\n",
    "        if w in syns:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "mac = load_macthes()\n",
    "mob = load_mobythes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mac thes DETER 27 entries:\n",
      "stop, stave off, foil, dissuade, prevent, intimidate, fend off, halt, check, obstruct, forestall, disincentivize, hinder, block, demoralize, inhibit, dishearten, daunt, counteract, put off, ward off, curb, discourage, impede, hamper, avert, scare off, \n",
      "\n",
      "mob thes DETER 64 entries:\n",
      "chill, preclude, stave off, stop, damp, turn away, distract, dissuade, prevent, forbid, intimidate, prohibit, indispose, fend off, blunt, cool, check, scare, divert, debar, keep from, obstruct, wean from, estop, dampen, restrain, disaffect, forestall, obviate, turn from, repel, deflect, foreclose, hinder, block, inhibit, dishearten, bar, ward, daunt, help, save, turn off, turn aside, anticipate, exclude, quench, put off, awe, keep off, ward off, disincline, overawe, discourage, impede, faze, frighten, disinterest, shake, fend, shut out, avert, scare off, rule out, "
     ]
    }
   ],
   "source": [
    "key = 'deter'\n",
    "print('mac thes', key.upper(), len(mac[key]), 'entries:')\n",
    "for w in mac[key]:\n",
    "    print(w, end=', ')\n",
    "print('\\n\\nmob thes', key.upper(), len(mob[key]), 'entries:')\n",
    "for w in mob[key]:\n",
    "    print(w, end=', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['glove-slim',\n",
       "  'word2vec-slim',\n",
       "  'arxiv_abs',\n",
       "  'nyt-science',\n",
       "  'arxiv_abs_lemma'],\n",
       " [<gensim.models.keyedvectors.Word2VecKeyedVectors at 0x10a9e6128>,\n",
       "  <gensim.models.keyedvectors.Word2VecKeyedVectors at 0x179cd5208>,\n",
       "  <gensim.models.keyedvectors.Word2VecKeyedVectors at 0x178215358>,\n",
       "  <gensim.models.keyedvectors.Word2VecKeyedVectors at 0x178a45d30>,\n",
       "  <gensim.models.keyedvectors.Word2VecKeyedVectors at 0x1803f3358>])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embd_nmes, embd_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERY: manage ( overlap: 0 )\n",
      "ARXIV_ABS_LEMMA : monitor, respond, allocate, communicate, HARNESS, designer, meet, protect, understand, request, \n",
      "THESAURUS: {'effect', 'control', 'direct', 'conduct', 'command', 'administer', 'succeed in', 'deal with', 'undertake', 'lead', 'preside over', 'do', 'bring about/off', 'engineer', 'perform', 'supervise', 'handle', 'guide', 'be at the helm of', 'carry out', 'master', 'organize', 'govern', 'superintend', 'head up', 'achieve', 'finish', 'be head of', 'contrive', 'cope with', 'run', 'oversee', 'rule', 'head', 'accomplish', 'be in charge of'}\n",
      "\n",
      "THESAURUS: {'bicycling', 'effect', 'keep', 'muddle through', 'operate', 'get by', 'discipline', 'make', 'pedaling', 'watch over', 'control', 'equitation', 'shape a course', 'conduct', 'dairy-farming', 'go on with', 'realize', 'administer', 'take care of', 'chicken-farming', 'consummate', 'come on', 'conn', 'succeed in', 'keep within compass', 'be such', 'play', 'exercise', 'feed', 'produce', 'stick it out', 'cycling', 'motoring', 'stand over', 'water', 'do', 'make out', 'engineer', 'tend', 'put through', 'brush', 'order', 'be so', 'supervise', 'handle', 'guide', 'captain', 'save', 'maneuver', 'negotiate', 'gnotobiotics', 'horsemanship', 'mastermind', 'put over', 'practice', 'put something aside', 'clear', 'go along', 'wield authority', 'chair', 'drench', 'head up', 'dispatch', 'officer', 'bed', 'cope with', 'manage somehow', 'curry', 'accomplish', 'employ', 'have the conn', 'cut the mustard', 'stockbreeding', 'carry through', 'tame', 'get by on', 'rub down', 'helm', 'keep at it', 'mink-ranching', 'complete', 'drive', 'never say die', 'pig-keeping', 'groom', 'train', 'knock off', 'stock raising', 'scrimp', 'milk', 'coxswain', 'manage with', 'animal rearing', 'bridle', 'scrape', 'transact', 'polish off', 'turn out', 'enforce economies', 'carry on', 'scrape along', 'scrape and save', 'make go', 'govern', 'put away', 'bring off', 'achieve', 'perform on', 'contrive', 'clear the hurdle', 'eke out', 'keep afloat', 'gentle', 'hitch', 'break', 'oversee', 'stack up', 'pull the strings', 'driving', 'execute', 'do the job', 'head', 'contend with', 'stagger', 'take the lead', 'treat', 'function', 'swing the deal', 'thremmatology', 'command', 'harness', 'enact', 'horse training', 'undertake', 'look after', 'lead', 'automobiling', 'succeed', 'animal husbandry', 'compass', 'cond', 'do with', 'fetch', 'herding', 'pilot', 'ordain', 'carry out', 'care for', 'go on', 'grazing', 'litter', 'fare', 'saddle', 'riding school', 'see to', 'swing', 'yoke', 'skimp', 'utilize', 'make use of', 'beekeeping', 'get along', 'take command', 'breeding', 'steer', 'economize', 'chart a course', 'busing', 'cattle-ranching', 'makeshift', 'worry along', 'stick to it', 'make ends meet', 'make the grade', 'call the signals', 'survive', 'dispose of', 'do the trick', 'get along on', 'skipper', 'work out', 'zootechny', 'bed down', 'direct', 'be master', 'bring about', 'see it through', 'steward', 'use', 'exert', 'turn the trick', 'make it', 'wield', 'prescribe', 'rule over', 'sheepherding', 'come along', 'motorcycling', 'deal with', 'attain', 'be responsible for', 'fodder', 'cope', 'discharge', 'hang in', 'make the rules', 'preside over', 'persevere', 'regulate', 'biking', 'perform', 'hang tough', 'manipulate', 'effectuate', 'quarterback', 'riding', 'subsist', 'lead on', 'administrate', 'fulfill', 'zootechnics', 'husband', 'dressage', 'navigate', 'superintend', 'work', 'get on', 'make do', 'persist', 'come through', 'bee culture', 'hack it', 'ply', 'shape up', 'run', 'horseback riding', 'currycomb', 'come out', 'dominate'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key = 'manage'\n",
    "embnme = 'arxiv_abs_lemma'\n",
    "response = simple_lookup(key, 'VERB', embkey=embnme, nopos=True)['words']  # no POS for arxiv_abs anyway\n",
    "print('QUERY:', key, '( overlap:', get_thes_count(key, response, mac), ')')\n",
    "print(embnme.upper(), ':', end=' ')\n",
    "for w in response:\n",
    "    if w in mac[key].union(mob[key]):\n",
    "        print(w.upper(), end=', ')\n",
    "    else:\n",
    "        print(w, end=', ')\n",
    "print('\\nTHESAURUS:', mac[key])\n",
    "print('\\nTHESAURUS:', mob[key])\n",
    "get_thes_count(key, response, mac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7., 0., 1., 9., 2., 0., 5., 0., 0., 3., 4., 2., 8., 2., 1., 1., 4.,\n",
       "       1., 4., 5., 3., 1., 4., 1., 2., 2., 1., 2., 1., 3.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_vocab(emb):\n",
    "    \"\"\"Return vocab of gensim emb object as set.\"\"\"\n",
    "    return set(emb.vocab.keys())\n",
    "\n",
    "def get_syn_meas(emb, thes, n=30, nn=10):\n",
    "    \"\"\"Randomly select n words from emb and return array of get_thes_count from top 10.\n",
    "    \n",
    "    :param emb: gensim emb ob\n",
    "    :param thes: dict of words to list of syn\n",
    "    :param n: number of words from emb vocab to randomly sample\n",
    "    :param nn: number of nearest neighbors to consider\n",
    "    :return counts: np array\n",
    "    \"\"\"\n",
    "    vocab = list(get_vocab(emb))\n",
    "    words = random.sample(vocab, n)\n",
    "    counts = np.zeros(n)\n",
    "    for i, w in enumerate(words):\n",
    "        while w not in thes:\n",
    "            w = random.sample(vocab, 1)[0]\n",
    "        response = [w for w, d in emb.most_similar(w, topn=20)]\n",
    "        counts[i] = get_thes_count(w, response, thes)\n",
    "    return counts\n",
    "\n",
    "get_syn_meas(embd_vecs[0], mob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove-slim     1.349          1.474          \n",
      "word2vec-slim  1.749          2.074          \n",
      "arxiv_abs      0.362          0.628          \n",
      "nyt-science    0.572          0.715          \n",
      "arxiv_abs_lem  0.466          0.650          \n"
     ]
    }
   ],
   "source": [
    "for emb, nme in zip(embd_vecs, embd_nmes):\n",
    "    syns = get_syn_meas(emb, mac, n=1000)\n",
    "    rels = get_syn_meas(emb, mob, n=1000)\n",
    "    print('{:<15}{:<15.3f}{:<15.3f}'.format(nme, np.mean(syns), np.mean(rels)))\n",
    "\n",
    "emb = gensim.models.KeyedVectors.load_word2vec_format('dat/vecs/arxiv_abs_lemma.txt')\n",
    "syns = get_syn_meas(emb, mac, n=1000)\n",
    "rels = get_syn_meas(emb, mob, n=1000)\n",
    "print('{:<15}{:<15.3f}{:<15.3f}'.format('arxiv_abs_lem', np.mean(syns), np.mean(rels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is distance a measure of goodness? --> NO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['glove-slim', 'word2vec-slim', 'arxiv_abs', 'nyt-science', 'arxiv_abs_lemma']\n",
      "\n",
      "\n",
      " DESIGN\n",
      "develop (0.60), devise (0.58), tailor (0.56), create (0.53), construct (0.53), implement (0.50), engineer (0.49), specialize (0.47), employ (0.47), designing (0.46), \n",
      "\n",
      " FUNCTIONALITY\n",
      "ethic (0.65), stakeholder (0.64), infrastructure (0.63), assistance (0.61), ecosystem (0.61), edward (0.60), automation (0.60), appraisal (0.60), functioning (0.60), platform (0.60), \n",
      "\n",
      " INSTRUMENTATION\n",
      "technologies (0.80), ehrs (0.80), cosmology (0.79), malaysia (0.79), japan (0.79), corporate (0.79), semiconductor (0.78), nsf (0.78), telemonitoring (0.78), usc (0.78), "
     ]
    }
   ],
   "source": [
    "# for emb, nme in zip(embd_vecs, embd_nmes):\n",
    "print(embd_nmes)\n",
    "words = ['design', 'functionality', 'instrumentation']\n",
    "emb = embd_vecs[-1]\n",
    "for w in words:\n",
    "    print('\\n\\n', w.upper())\n",
    "    for t in emb.most_similar(w):\n",
    "        print('{} ({:.2f}),'.format(t[0], t[1]), end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
